# -*- coding: utf-8 -*-
"""NN_final_maskRCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DhnbJXAwnc7rzmOLahdMMpsiFSAro7tx
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow

!pip install keras==2.2.5

# Commented out IPython magic to ensure Python compatibility.
import os
import sys
import random
import math
import json
import re
import time
import numpy as np
import cv2
import matplotlib
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw
import tensorflow as tf
# Root directory of the project
ROOT_DIR = '/content/drive/My Drive/neural_final/MaskRCNN/Mask_RCNN-master/'

# Import Mask RCNN
sys.path.append(ROOT_DIR)  # To find local version of the library
from mrcnn.config import Config
from mrcnn import utils
import mrcnn.model as modellib
from mrcnn import visualize
from mrcnn.model import log

# %matplotlib inline 

# Directory to save logs and trained model
MODEL_DIR = '/content/drive/My Drive/neural_final/MaskRCNN/logs'
# MODEL_DIR = os.path.join(ROOT_DIR, "logs")

# Local path to trained weights file
COCO_MODEL_PATH = '/content/drive/My Drive/neural_final/MaskRCNN/mask_rcnn_coco.h5'
#os.path.join(ROOT_DIR, "mask_rcnn_coco.h5")
# Download COCO trained weights from Releases if needed
if not os.path.exists(COCO_MODEL_PATH):
    utils.download_trained_weights(COCO_MODEL_PATH)



class modelConfig(Config):  #
    """Configuration for training on the cigarette butts dataset.
    Derives from the base Config class and overrides values specific
    to the cigarette butts dataset.
    """
    # Give the configuration a recognizable name
    NAME = "plant_phenotyping"

    # Train on 1 GPU and 1 image per GPU. Batch size is 1 (GPUs * images/GPU).
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1

    # Number of classes (including background)
    NUM_CLASSES = 1 + 2  # background + 1 (cig_butt)

    # All of our training images are 512x512
    IMAGE_MIN_DIM = 512
    IMAGE_MAX_DIM =  512

    # You can experiment with this number to see if it improves training
    STEPS_PER_EPOCH = 500

    # This is how often validation is run. If you are using too much hard drive space
    # on saved models (in the MODEL_DIR), try making this value larger.
    VALIDATION_STEPS = 5
    
    # Matterport originally used resnet101, but I downsized to fit it on my graphics card
    BACKBONE = 'resnet101'

    # To be honest, I haven't taken the time to figure out what these do
    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)
    TRAIN_ROIS_PER_IMAGE = 32
    MAX_GT_INSTANCES = 50 
    POST_NMS_ROIS_INFERENCE = 500 
    POST_NMS_ROIS_TRAINING = 1000 
    
config = modelConfig()
config.display()

def get_ax(rows=1, cols=1, size=8):
    """Return a Matplotlib Axes array to be used in
    all visualizations in the notebook. Provide a
    central point to control graph sizes.
    
    Change the default size attribute to control the size
    of rendered images
    """
    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))
    return ax

class CocoLikeDataset(utils.Dataset):
    """ Generates a COCO-like dataset, i.e. an image dataset annotated in the style of the COCO dataset.
        See http://cocodataset.org/#home for more information.
    """
    def load_data(self, annotation_json, images_dir):
        """ Load the coco-like dataset from json
        Args:
            annotation_json: The path to the coco annotations json file
            images_dir: The directory holding the images referred to by the json file
        """
        # Load json from file
        json_file = open(annotation_json)
        coco_json = json.load(json_file)
        json_file.close()
        
        # Add the class names using the base method from utils.Dataset
        source_name = "coco_like"
        for category in coco_json['categories']:
            class_id = category['id']
            class_name = category['name']
            if class_id < 1:
                print('Error: Class id for "{}" cannot be less than one. (0 is reserved for the background)'.format(class_name))
                return
            
            self.add_class(source_name, class_id, class_name)
        
        # Get all annotations
        annotations = {}
        for annotation in coco_json['annotations']:
            image_id = annotation['image_id']
            if image_id not in annotations:
                annotations[image_id] = []
            annotations[image_id].append(annotation)
        
        # Get all images and add them to the dataset
        seen_images = {}
        for image in coco_json['images']:
            image_id = image['id']
            if image_id in seen_images:
                print("Warning: Skipping duplicate image id: {}".format(image))
            else:
                seen_images[image_id] = image
                try:
                    image_file_name = image['file_name']
                    image_width = image['width']
                    image_height = image['height']
                except KeyError as key:
                    pass
#                     print("Warning: Skipping image (id: {}) with missing key: {}".format(image_id, key))
                
                image_path = '/content/drive/My Drive/data_resized'+image_file_name
                # os.path.abspath(os.path.join('/content/drive/My Drive/neural_final/data', image_file_name))
                image_annotations = annotations[image_id]
                
                
                # Add the image using the base method from utils.Dataset
                self.add_image(
                    source=source_name,
                    image_id=image_id,
                    path=image_path,
                    width=512,   #image_width,
                    height=512,   #image_height,
                    annotations=image_annotations
                )
                
    def load_mask(self, image_id):
        """ Load instance masks for the given image.
        MaskRCNN expects masks in the form of a bitmap [height, width, instances].
        Args:
            image_id: The id of the image to load masks for
        Returns:
            masks: A bool array of shape [height, width, instance count] with
                one mask per instance.
            class_ids: a 1D array of class IDs of the instance masks.
        """
        image_info = self.image_info[image_id]
        annotations = image_info['annotations']
        instance_masks = []
        class_ids = []
        
        for annotation in annotations:
            class_id = annotation['category_id']
            mask = Image.new('1', (image_info['width'], image_info['height']))
            mask_draw = ImageDraw.ImageDraw(mask, '1')
            #for segmentation in annotation['keypoints']:
            
            keypoint = annotation['keypoints']
            # keypoint.pop(2)
#             keypoint.pop(4)
            keypoint = keypoint[0:4]
#             print(keypoint)
            mask_draw.rectangle(keypoint, fill=1, width=2)
            bool_array = np.array(mask) > 0
            instance_masks.append(bool_array)
            class_ids.append(class_id)

        mask = np.dstack(instance_masks)
        class_ids = np.array(class_ids, dtype=np.int32)
        
        return mask, class_ids

dataset_train = CocoLikeDataset()
dataset_train.load_data('/content/drive/My Drive/neural_final/train_resize.json', '/content/drive/My Drive/data_resized')
dataset_train.prepare()

dataset_val = CocoLikeDataset()
dataset_val.load_data('/content/drive/My Drive/neural_final/test_resize.json', '/content/drive/My Drive/data_resized')
dataset_val.prepare()

dataset = dataset_train
image_ids = np.random.choice(dataset.image_ids, 4)
for image_id in image_ids:
    image = dataset.load_image(image_id)
    mask, class_ids = dataset.load_mask(image_id)
    visualize.display_top_masks(image, mask, class_ids, dataset.class_names)

# Create model in training mode
model = modellib.MaskRCNN(mode="training", config=config,
                          model_dir=MODEL_DIR)

# Which weights to start with?
init_with = "coco"  # imagenet, coco, or last

if init_with == "imagenet":
    model.load_weights(model.get_imagenet_weights(), by_name=True)
elif init_with == "coco":
    # Load weights trained on MS COCO, but skip layers that
    # are different due to the different number of classes
    # See README for instructions to download the COCO weights
    model.load_weights(COCO_MODEL_PATH, by_name=True,
                       exclude=["mrcnn_class_logits", "mrcnn_bbox_fc", 
                                "mrcnn_bbox", "mrcnn_mask"])
elif init_with == "last":
    # Load the last model you trained and continue training
    model.load_weights(model.find_last(), by_name=True)

# Passing layers="heads" freezes all layers except the head
# layers. You can also pass a regular expression to select
# which layers to train by name pattern.
model.train(dataset_train, dataset_val, 
            learning_rate=config.LEARNING_RATE, 
            epochs=5, 
            layers='heads')

# Fine tune all layers
# Passing layers="all" trains all layers. You can also 
# pass a regular expression to select which layers to
# train by name pattern.
model.train(dataset_train, dataset_val, 
            learning_rate=config.LEARNING_RATE / 10,
            epochs=50, 
            layers="all")

hist = model.keras_model.history.history
# plt.plot(hist.history['acc'])
# plt.plot(hist.history['val_acc'])
# plt.title('Model accuracy')
# plt.ylabel('Accuracy')
# plt.xlabel('Epoch')
# plt.legend(['Train', 'Val'], loc='upper left')
# plt.show()

plt.plot(hist['val_loss'])
plt.title('Validation loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
# plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

plt.plot(hist['val_rpn_class_loss'])
plt.title('Validation RPN Class loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.show()

plt.plot(hist['val_rpn_bbox_loss'])
plt.title('Validation RPN Bounding box loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.show()

plt.plot(hist['val_mrcnn_class_loss'])
plt.title('Validation Mask RCNN Class loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.show()

plt.plot(hist['val_mrcnn_bbox_loss'])
plt.title('Validation Mask RCNN Bounding box loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.show()

plt.plot(hist['val_mrcnn_mask_loss'])
plt.title('Validation Mask RCNN mask loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.show()

plt.plot(hist['loss'])
plt.title('Loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.show()

plt.plot(hist['rpn_class_loss'])
plt.title('RPN Class Loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.show()

plt.plot(hist['rpn_bbox_loss'])
plt.title('RPN Bounding box loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.show()

plt.plot(hist['mrcnn_class_loss'])
plt.title('Mask RCNN Class loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.show()

plt.plot(hist['mrcnn_bbox_loss'])
plt.title('Mask RCNN Bounding box loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.show()

plt.plot(hist['mrcnn_mask_loss'])
plt.title('Mask RCNN mask loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.show()

class InferenceConfig(modelConfig):
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1

inference_config = InferenceConfig()

# Recreate the model in inference mode
model = modellib.MaskRCNN(mode="inference", 
                          config=inference_config,
                          model_dir=MODEL_DIR)

# Get path to saved weights
# Either set a specific path or find last trained weights
# model_path = os.path.join(ROOT_DIR, ".h5 file name here")
model_path = model.find_last()

# Load trained weights
print("Loading weights from ", model_path)
model.load_weights(model_path, by_name=True)

# Test on a random image
image_id = random.choice(dataset_val.image_ids)
original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\
    modellib.load_image_gt(dataset_val, inference_config, 
                           image_id, use_mini_mask=False)

log("original_image", original_image)
log("image_meta", image_meta)
log("gt_class_id", gt_class_id)
log("gt_bbox", gt_bbox)
log("gt_mask", gt_mask)

visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, 
                            dataset_train.class_names, figsize=(8, 8))
print(gt_bbox)

results = model.detect([original_image], verbose=1)
# print(results)
# results['masks']
r = results[0]
print(r['rois'])
visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], 
                            dataset_val.class_names, r['scores'], ax=get_ax())

# Compute VOC-Style mAP @ IoU=0.5
# Running on 10 images. Increase for better accuracy.

image_ids = np.random.choice(dataset_val.image_ids, 50)
APs = []
for image_id in image_ids:
    # Load image and ground truth data
    image, image_meta, gt_class_id, gt_bbox, gt_mask =\
        modellib.load_image_gt(dataset_val, inference_config,
                               image_id, use_mini_mask=False)
    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)
    # Run object detection
    results = model.detect([image], verbose=0)
    r = results[0]
    # Compute AP
    AP, precisions, recalls, overlaps =\
        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,
                         r["rois"], r["class_ids"], r["scores"], r['masks'])
    APs.append(AP)
    bbox_over = utils.compute_overlaps(gt_bbox, r["rois"])
    # print(bbox_over[0])
    # recall = utils.compute_recall
# print(recalls)
print("mAP: ", np.mean(APs))
print(APs)

visualize.plot_precision_recall(AP, precisions, recalls)

visualize.plot_overlaps(gt_class_id, r['class_ids'], r['scores'],
                        overlaps, dataset.class_names)